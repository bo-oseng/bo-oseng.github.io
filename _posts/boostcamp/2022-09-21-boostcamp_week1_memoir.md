---
layout: single

title: 부스트캠프에 AI 1일차 ~ 3일차 회고록  
categories:
  - Boostcamp

tag: [Boostcamp, 부스트캠프, ]

toc: true

---

## Day 1

+ 첫날인 만큼 타운홀미팅(오리엔테이션)이 진행됐다.

+ 타운홀미팅이 무슨뜻일까 궁금해서 찾아봤는데 정치인이 지역구 주민들과 만나 의견을 경청하는 미팅이라고 한다.

+ 모더레이터, 피어세션, 데일리스크럼 등등 낯선 용어가 많았다. 운영진분들도 네이밍하기 힘들겠다...ㅋㅋ

+ 오리엔테이션인 만큼 부스트캠프의 전체적인 흐름에대해 설명해주셨다.

+ level 1 8주동안 함께할 동료들을 만났다. 든든하다.

+ 유사 역행렬인 무어 펜로즈 행렬에 관해서 학습했다. 선형회귀는 유사 역행렬을 푸는것 만으로도 풀이가 가능하다. 단 비선형 함수는 불가능하다.

+ 선형 함수를 위해서는 경사하강법 즉 Gradient Descent가 쓰인다.

+ 심화과제에서 Gradient Descent를 학습하면서 sympy로 다항식을 세우고 미분하는 법에 대해 배웠다.

+ Gradient Descent를 직접 알고리즘으로 구현하면서 더 깊게 이해하게 되었다.   



## Day 2

+ 경사하강법에 매운맛을 수강했다. 

     $$ \left\|y - X \beta\right\|$$
  +  위와 같은 손실함수의 L2_norm을 구할때 제곱합의 평균의 제곱근을 Root Mean Square Error 줄여서 RMSE라 한다.

  + 위와 같은 손실함수의 L2_norm은 제곱합의 평균 Mean Square Error로 접근하는게 미분할때 더 쉽게 계산할 수 있다.(제곱근 안의 수가 0에 가까워 지는 방향과 MSE 자체가 0에 가까워지는 방향은 같다.)

    $$ \nabla_\beta\left\|y - X \beta\right\| = \frac{\partial{MSE}}{\partial\beta} =\delta_\beta\{\frac1n \sum_{i=1}^n(y_i - \sum_{j=1}^dX_{ij}\beta_{j})^2\}$$

    $$ = -\frac{2}{n}X^{T}(y-X\beta^{(t)}) $$

+ 경사하강법은 미분가능하고 conves(볼록)한 함수에 대해선 적절한 학습률과 학습 횟수를 선태했을때 이론적으로 항상 수렴이 보장되어 있다.

+ 유용한 사이트를 발견했다. [데이터 사이언스 스쿨](https://datascienceschool.net/intro.html)

+ Stochastic Gradient Descent(확률적 경사 하강법)에 대해 학습했다. 데이터 중 일부만을 활용해 업데이트를 하는 방법이다. 
    + SGD는 데이터의 일부로 파라미터를 업데이트 하므로 연산자원을 좀 더 효율적으로 활용할 수 있다.
    + 데이터의 일부를 미니배치라 하는데 미니배치는 에폭마다 미니배치를 확률적으로 선택해 모델을 학습하므로 목적식 모양이 바뀌게 된다.
    + SGD는 볼록이 아닌 목적식에서도 사용이 가능하다.
    + 정확한 gradient를 구해서 움직이는 방법이 아니라 정확한 그래디언트 방향으로만의 이동이 이루어 지지 않고 왔다갔다 하는 경향이 있다.
    + SGD가 만능은 아니지만 실증적으로 경사하강법보다 더 낫다고 검증 되었다.

+ 하드웨어 입장에서도 SGD는 모든 데이터를 한번에 메모리에 업로드하지 않고 데이터의 일부를 미니배치로 쪼개 메모리에 올려 계산하므로 원본 데이터가 크더라도 좀더 out-of-memory로 부터 자유롭다.


### Day 3

+ softmax 함수에 대해 학습했다.
+ 추론을 할때는 one-hot 벡터를 분류를 학습 할때는 softmax를 사용하자.

+ 딥러닝 학습방법 이해하기를 수강했다. 비선형 모델인 neural network에 대해 학습했다. 


    $$ \vec{o}_{({n}\times{p})} ={\vec{x}_{({n}\times{d})}}\times{ \mathbb{W}_{({d}\times{p})}} + \vec{b}_{({n}\times{p})} $$

  + 위 식은 지금까지 다뤄왔던 선형모델이고 그려보면 다음과 같다.

    <img src="https://user-images.githubusercontent.com/94548914/191547199-77cd864b-4c9c-4374-a62b-c5f6d1d7af77.png">

  + d 개의 변수로 p개의 선형모델을 만들어서 p개의 잠재변수를 설명하는 모델이라고 할 수 있다.
  + 화살표들의 개수가 총 (d x p)개이고 이는 가중치 행렬의 성분수와 같다.
  + 기존까지 다뤄왔던 선형모델의 각각에 비선형 함수를 합성한다. 
    $$ \mathbb{H} = (\sigma(z_1), \sigma(z_1), ..., \sigma(z_n)) $$
    $$ \sigma(\vec{z}) = \sigma(\mathbb{W}\vec{x} + \vec{b}) $$
  + 위 식을 모델로 그려보면 다음과 같다.
  <img src="https://user-images.githubusercontent.com/94548914/191549251-1f83886a-c1d0-4f34-b803-69e78c35d3e8.png">
  + 각각의 z<sub>1</sub>, z<sub>2</sub>, ..., z<sub>n</sub>에 각각 비선형 함수를 합성해야한다.
  + z 벡터에 비선형함수를 합성해 새로만든 잠재벡터를 Hidden Vector라고도 하고 H 벡터라한다.
  + 비선형 모델을 합성하므로서 선형모델만 다룰 수 있던 한계를 극복할 수 있다.
  + 네모 상자 안에 있는 벡터들을 퍼셉트론 혹은 뉴런이라 하고 뉴런으로 이루어진 모델을 neural network한다.
  + 히든벡터 층이 2개 있으면 Two Layer Perceptron 여러개 있으면 Multi Layer Percetropn 이라한다.
  + 이론적으로 2층 신경망으로도 임의의 연속함수를 근사할 수 있다.
  + 그러나 층이 깊을수록 뉴런의 숫자가 더 빨리 줄어 들어 좀더 효유적이다.
  + 층이 깊다고해서 최적화가 쉬운것은 아니다. 깊어질수록 최적화 하기는 더 어려워진다.
+ 대표적인 활성함수는 Sigmoid, tanh, ReLu 등이 있다.
+ 딥러닝에선 ReLu 함수를 많이쓴다.